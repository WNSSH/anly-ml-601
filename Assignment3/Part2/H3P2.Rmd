---
title: "H3P2"
author: "Guanzhi Wang"
date: "4/10/2020"
output: pdf_document
---

# Question 3
```{r}
####################################################
# Class: Anly-601
# Script: Starter code for embedding
# Author: Joshuah Touyz
# Version: 0.1
# Last updated: 03/28/20
####################################################

####################################
#####     Install libraries     #####
####################################
install.packages("devtools")
devtools::install_github("bmschmidt/wordVectors")

####################################
##### Loading libraries & data #####
####################################
library(wordVectors)
library(Rtsne)
library(tidytext)
library(tidyverse)
```


```{r}
####################################
#####       Download data      #####
####################################
# -- Check to see  if file exists --
if (!file.exists("cookbooks.zip")) {
  download.file("http://archive.lib.msu.edu/dinfo/feedingamerica/cookbook_text.zip","cookbooks.zip")
}
unzip("cookbooks.zip",exdir="cookbooks")
if (!file.exists("cookbooks.txt")) prep_word2vec(origin="cookbooks",destination="cookbooks.txt",lowercase=T,bundle_ngrams=2)

# Training a Word2Vec model
if (!file.exists("cookbook_vectors.bin")) {
  model = train_word2vec("cookbooks.txt","cookbook_vectors.bin",
                         vectors=100,threads=4,window=6,
                         min_count = 10,
                         iter=5,negative_samples=15)
} else{
    model = read.vectors("cookbook_vectors.bin")
    }

####################################
#####      Proximity search    #####
####################################

# -- Select ingredient and cuisine --
ingredient = 'tomato'
ingredient_2 = 'onion'
ingredient_3 = 'carrot'
list_of_ingredients = c(ingredient, ingredient_2, ingredient_3)
cuisine = 'italian'

# Coordinages in 300D space of embedding for the word "sage" 
model[[ingredient]]

# Searching closest words to sage
model %>% closest_to(model[[ingredient]],5) #<- set of closest ingredients to "sage"
model %>% closest_to(model[[cuisine]], 20) #<- set of closest cuisines to "italian"

# Set of closest words to "sage", "thyme","basil"
model %>% closest_to(model[[list_of_ingredients]],10)

#############################################
#####   Using TSNE to see similarity    #####
#############################################
# We have a list of potential herb-related words from old cookbooks. 
n_words = 100
closest_ingredients = closest_to(model,model[[list_of_ingredients]], n_words)$word
surrounding_ingredients = model[[closest_ingredients,average=F]]
plot(surrounding_ingredients,method="pca")

embedding = Rtsne(X = surrounding_ingredients, dims = 2, 
                  perplexity = 4, 
                  theta = 0.5, 
                  eta = 10,
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 2000)
embedding_vals = embedding$Y
rownames(embedding_vals) = rownames(surrounding_ingredients)

# Looking for clusters for embedding
set.seed(10)
n_centers = 10
clustering = kmeans(embedding_vals,centers=n_centers,
                    iter.max = 5)

# Setting up data for plotting
embedding_plot = tibble(x = embedding$Y[,1], 
                        y = embedding$Y[,2],
                        labels = rownames(surrounding_ingredients)) %>% 
  bind_cols(cluster = as.character(clustering$cluster))

# Visualizing TSNE output
ggplot(aes(x = x, y=y,label = labels, color = cluster), data = embedding_plot) + 
  geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"')+theme(legend.position = 'none')

# Topics produced by the top 3 words
sapply(sample(1:centers,n_centers),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:10])
})

##################################################
#####   Plotting Sweet and Salty Dimensions  #####
##################################################
# -- Plotting across the sweet-salty plane --
tastes = model[[c("sweet","salty"),average=F]]
sweet_and_saltiness = model[1:500,] %>% cosineSimilarity(tastes)

# Filter to the top n words for sweet or salty.
top_n_words = 10
sweet_and_saltiness = sweet_and_saltiness[
  rank(-sweet_and_saltiness[,1])<top_n_words | 
    rank(-sweet_and_saltiness[,2])<top_n_words,
  ]
plot(sweet_and_saltiness,type='n')
text(sweet_and_saltiness,labels=rownames(sweet_and_saltiness))

###########################################
#####   Plotting 5 Taste  Dimensions  #####
###########################################
# We can plot along mltiple dimensions:
tastes = c("hot","sour","spicy")
common_similarities_tastes = model[1:3000,]%>% cosineSimilarity( model[[tastes,average=F]])
high_similarities_to_tastes = common_similarities_tastes[rank(-apply(common_similarities_tastes,1,max)) < 20,]

# - Plotting
high_similarities_to_tastes %>% 
  as_tibble(rownames='word') %>%
  filter( ! (is.element(word,set_of_tastes))) %>%
  #mutate(total = salty+sweet+savory+bitter+sour) %>%
  #mutate( sweet=sweet/total,salty=salty/total,savory=savory/total,bitter=bitter/total, sour = sour/total) %>% 
  #select(-total) %>%
  gather(key = 'key', value = 'value',-word) %>%
  ggplot(aes(x = word,
             y = value, 
             fill = key)) + geom_bar(stat='identity') + 
  coord_flip() + theme_minimal() + scale_fill_brewer(palette='Spectral')


# --- Most similar terms  ---
high_similarities_to_tastes %>% 
  prcomp %>% 
  biplot(main="Fifty words in a\nprojection of flavor space")

##################################
#####   Vector calculations  #####
##################################
model %>% closest_to("cookie") # words associated with haelthy living (if not a bit outdated)
model %>% closest_to(~("cookie" - "fish" ),15) # number 7 is cravings
model %>% closest_to(~"cookie" + ("fish"- "sweet"),15)

model %>% closest_to(~"chinese" + ("beef" - "lamb"),15)

top_evaluative_words = model %>% 
  closest_to(~ "poached"+"florentine",n=30)
goodness = model %>% 
  closest_to(~ "poached"-"florentine",n=Inf) 
taste = model %>% 
  closest_to(~ "egg" - "spinach", n=Inf)

top_evaluative_words %>%
  inner_join(goodness) %>%
  inner_join(taste) %>%
  ggplot() + 
  geom_text(aes(x=`similarity to "poached" - "florentine"`,
                y=`similarity to "egg" - "spinach"`,
                label=word))



```


##Question 4

```{r}
df = read.csv('kernel_regression_1.csv')
set.seed(1314)
idx<- sample(seq_len(1001), size =20)
x_obv = df$x[-idx]
f=df$y[-idx]
x_prime=df$x

K = function(x,x_prime,theta){
  d = sapply(x,FUN=function(x_in)(x_in-x_prime)^2)
  return(t(exp(-1/(2*theta)*d)))
}
mu=0
mu_star=0
theta=0.001

K_f = K(x_obv,x_obv,theta)
for (i in 1:dim(K_f)[1]){
  K_f[i,i]=K_f[i,i]+0.01
}
K_star = K(x_obv,x_prime,theta)
K_starstar = K(x_prime,x_prime,theta )
mu_star = mu_star + t(K_star)%*%solve(K_f)%*%(f-mu)
Sigma_star = K_starstar - t(K_star)%*%t(solve(K_f))%*%K_star
```

```{r}
model_fit<-function(theta)
{
  K_f = K(x_obv,x_obv,theta)
  for (i in 1:dim(K_f)[1]){
    K_f[i,i]=K_f[i,i]+0.01
  }
  K_star = K(x_obv,x_prime,theta)
  K_starstar = K(x_prime,x_prime,theta )
  mu_star = mu_star + t(K_star)%*%solve(K_f)%*%(f-mu)
  Sigma_star = K_starstar - t(K_star)%*%t(solve(K_f))%*%K_star
  Sigma_star_test = Sigma_star[idx,idx]
  logLL = -log(det(Sigma_star_test)) - t(df$y[idx]-mu_star[idx])%*%solve(Sigma_star_test)%*%(df$y[idx]-mu_star[idx])
  cat('theta=',theta,'Negative Log Likelihood is',-logLL,'\n')}

for (theta in c(0.005,0.01,0.05,0.1,0.125,0.5,1)){
  model_fit(theta)
}
```

```{r}
plot_gp = tibble(x = x_prime, 
                 y = mu_star %>% as.vector(),
                 sd_prime = sqrt(diag(Sigma_star)))

ggplot(aes(x = x, y = y), data = plot_gp) + 
  geom_line()+ 
  geom_ribbon(aes(ymin = y-sd_prime,ymax = y+sd_prime),color='yellow')+
  geom_point(aes(x =x , y= y), data = tibble(x = x_obv, y = f), alpha=0.5,
             color = 'grey') +
  xlim(c(-4,4))+ylim(c(-3,3))+coord_fixed(ratio = 1) +ylab('f(x)')
```

```{r}
df = read.csv('kernel_regression_2.csv')

set.seed(1314)
idx<- sample(seq_len(1001), size =20)
x_obv = df$x[-idx]
f=df$y[-idx]
x_prime=df$x

K = function(x,x_prime,theta){
  d = sapply(x,FUN=function(x_in)(x_in-x_prime)^2)
  return(t(exp(-1/(2*theta)*d)))
}
mu=0
mu_star=0
theta=0.001

K_f = K(x_obv,x_obv,theta)
for (i in 1:dim(K_f)[1]){
  K_f[i,i]=K_f[i,i]+0.01
}
K_star = K(x_obv,x_prime,theta)
K_starstar = K(x_prime,x_prime,theta )
mu_star = mu_star + t(K_star)%*%solve(K_f)%*%(f-mu)
Sigma_star = K_starstar - t(K_star)%*%t(solve(K_f))%*%K_star
```

```{r}
model_fit<-function(theta)
{
  K_f = K(x_obv,x_obv,theta)
  for (i in 1:dim(K_f)[1]){
    K_f[i,i]=K_f[i,i]+0.01
  }
  K_star = K(x_obv,x_prime,theta)
  K_starstar = K(x_prime,x_prime,theta )
  mu_star = mu_star + t(K_star)%*%solve(K_f)%*%(f-mu)
  Sigma_star = K_starstar - t(K_star)%*%t(solve(K_f))%*%K_star
  Sigma_star_test = Sigma_star[idx,idx]
  logLL = -log(det(Sigma_star_test)) - t(df$y[idx]-mu_star[idx])%*%solve(Sigma_star_test)%*%(df$y[idx]-mu_star[idx])
  cat('theta=',theta,'Negative Log Likelihood is',-logLL,'\n')}

for (theta in c(0.001,0.01,0.05,0.1,0.125)){
  model_fit(theta)
}
```

```{r}
plot_gp = tibble(x = x_prime, 
                 y = mu_star %>% as.vector(),
                 sd_prime = sqrt(diag(Sigma_star)))

ggplot(aes(x = x, y = y), data = plot_gp) + 
  geom_line()+ 
  geom_ribbon(aes(ymin = y-sd_prime,ymax = y+sd_prime),color='yellow')+
  geom_point(aes(x =x , y= y), data = tibble(x = x_obv, y = f), alpha=0.5,
             color = 'grey') +
  xlim(c(-4,4))+ylim(c(-3,3))+coord_fixed(ratio = 1) +ylab('f(x)')
```

